{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            Stock Analysis and Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "    1. Introduction\n",
    "    2. Importing libraries\n",
    "    3. Reading datasets\n",
    "    4. Building Models\n",
    "    5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project we will show how to write a python program that predicts the price of stocks using a machine learning technique called Long Short-Term Memory (LSTM) as well as create a optimize portfoilo using Efficient Frontier.\n",
    "\n",
    "We will be solve the following question:\n",
    "\n",
    "1. What was the change in price of the stock over time?\n",
    "2. What was the monthly return of the stock on average?\n",
    "3. What was the moving average of the various stocks?\n",
    "4. What was the correlation between different stocks'?\n",
    "5. How much value do we put at risk by investing in a particular stock?\n",
    "6. Portfoilo optimization using Efficient Frontier?\n",
    "7. How can we attempt to predict future stock behavior using LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are importing important libraries\n",
    "import math\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as sco\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from pandas_datareader import data as web\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Companies names and thier Ticker that we used for our Analysis\n",
    "    \n",
    "    1. Apple Inc. = AAPL\n",
    "    2. Alphabet Inc. = GOOG\n",
    "    3. Microsoft Corporation = MSFT\n",
    "    4. Amazon = AMZN\n",
    "    5. Facebook Inc. = FB\n",
    "    6. Alibaba Group = BABA\n",
    "    7. Johnson & Johnson = JNJ\n",
    "    8. JPMorgon Chase & Co. = JPM\n",
    "    9. ExxonMobil = XOM\n",
    "    10.Bank of America = BAC\n",
    "    11.WalMart Store Inc. = WMT\n",
    "    12.Wells Fargo & Co. = WFC\n",
    "    13.Visa Inc. = V\n",
    "    14.Procter & Gamble Co. = PG\n",
    "    15.Verizon Communication = VZ\n",
    "    16.AT&T Inc. = T\n",
    "    17.UnitedHealth Group Inc. = UNH\n",
    "    18.Home Depot = HD\n",
    "    19.Intel = INTC\n",
    "    20.Oracle = ORCL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What was the change in price of the stock over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of ticker of companies\n",
    "ticker_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'FB', 'BABA','JNJ', 'JPM', 'XOM', 'BAC', 'WMT', 'WFC', 'V', 'PG', 'VZ', 'T', 'UNH', 'HD', 'INTC', 'ORCL']\n",
    "data = pdr.get_data_yahoo(ticker_list, start = '2015-01-01')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Monthly Adjused Close of all companies\n",
    "monthly_adjusted_close = data['Adj Close'].resample('M').ffill()\n",
    "monthly_adjusted_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is quick summary of each company Monthly Adjusted closing\n",
    "monthly_adjusted_close.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Monthly volume of all companies\n",
    "monthly_volume = data['Volume'].resample('M').ffill()\n",
    "monthly_volume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What was the monthly return of the stock on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating monthly return\n",
    "monthly_returns = data['Adj Close'].resample('M').ffill().pct_change()\n",
    "monthly_returns.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are visualising of Monthly Adjusted Price, Monthly Volume and Monthly % Change\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 20, ncols = 3)\n",
    "fig.suptitle('Monthly Adjusted Closing VS Monthly Volume VS Monthly % Change')\n",
    "fig.set_figheight(40)\n",
    "fig.set_figwidth(15)\n",
    "#plt.subplots_adjust(top=2.25, bottom=2.2)\n",
    "\n",
    "columns = list(monthly_adjusted_close) \n",
    "  \n",
    "for i, cols in enumerate(columns,0):\n",
    "    monthly_adjusted_close[cols].plot(ax = axes[i,0])\n",
    "    axes[i,0].set(xlabel='Date', ylabel='Adj Close')\n",
    "    axes[i,0].set_title(f\"{ticker_list[i]}\")\n",
    "    monthly_volume[cols].plot(ax = axes[i,1])\n",
    "    axes[i,1].set(xlabel='Date', ylabel='Volume')\n",
    "    axes[i,1].set_title(f\"{ticker_list[i]}\")\n",
    "    monthly_returns[cols].plot(ax = axes[i,2])\n",
    "    axes[i,2].set(xlabel='Date', ylabel='Daily % change')\n",
    "    axes[i,2].set_title(f\"{ticker_list[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(columns, monthly_returns.mean())\n",
    "plt.xlabel('Company Name')\n",
    "plt.ylabel('Average monthly return')\n",
    "plt.suptitle('Average Monthly Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five = monthly_returns.mean().nlargest(5)\n",
    "company_name = list(top_five.keys())\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.bar(company_name, top_five)\n",
    "plt.suptitle('Top 5 companies with highest Monthly Return')\n",
    "plt.ylabel('Average Monthly return')\n",
    "plt.xlabel('Company name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've brock down our companies from 20 to 5 on the basic of average percentage change in stock price and also seen the visualizations for the Adjusted price and the volume traded each day with daily percentage change, let's go ahead and caculate the moving average for the stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What was the moving average of the various stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three important moving averages that we have applied to our charts so that it will help us to trade better. They are the 10 moving average, the 20 moving average and the 50 moving average. The 20 moving average (10MA) is the short-term outlook. The 50 moving average (20MA) is the medium term outlook. The 200 moving average (50MA) is the trend bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are using for loop for grabing yahoo data and setting it in form of dataframe\n",
    "#  Using globals() is a sloppy way of setting the DataFrame names, but its simple\n",
    "for stock in company_name:\n",
    "    globals()[stock] = web.DataReader(stock,\"yahoo\",'2013-01-01',datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = [AMZN,MSFT,AAPL,FB,BABA]\n",
    "for company, comp_name in zip(company_list,company_name):\n",
    "    company[\"company_name\"] = comp_name\n",
    "    \n",
    "company_data = pd.concat(company_list,axis=0)\n",
    "company_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ma_day = [10, 20, 50]\n",
    "\n",
    "for ma in ma_day:\n",
    "    for company in company_list:\n",
    "        column_name = f\"MA for {ma} days\"\n",
    "        company[column_name] = company['Adj Close'].rolling(ma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are visualising the additional moving averages\n",
    "company_data.groupby(\"company_name\").hist(figsize=(12, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are visualising three important moving averages of all the company\n",
    "fig, axes = plt.subplots(nrows=3,ncols=2)\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "AMZN[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,0])\n",
    "axes[0,0].set_title('AMAZON')\n",
    "\n",
    "AAPL[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,1])\n",
    "axes[0,1].set_title('APPLE')\n",
    "\n",
    "FB[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,0])\n",
    "axes[1,0].set_title('FACEBOOK')\n",
    "\n",
    "MSFT[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,1])\n",
    "axes[1,1].set_title('MICROSOFT')\n",
    "\n",
    "BABA[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[2,0])\n",
    "axes[2,0].set_title('UnitedHealth')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use pct_change to find the percent change for each month\n",
    "for company in company_list:\n",
    "    company['Monthly Return'] = company['Adj Close'].resample('M').ffill().pct_change()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i, company in enumerate(company_list, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.distplot(company['Monthly Return'].dropna(), bins=100, color='blue')\n",
    "    plt.ylabel('Monthly Return')\n",
    "    plt.title(f'{company_name[i - 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What was the correlation between different stocks'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Top five company Monthly Adjusted Close price\n",
    "adjusted_close_five= monthly_adjusted_close[company_name]\n",
    "adjusted_close_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are Making DataFrame which is Monthly % change\n",
    "returns_five= adjusted_close_five.pct_change()\n",
    "returns_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are comparing Amazon to itself should show a perfectly linear relationship\n",
    "sns.jointplot('AMZN', 'AMZN', returns_five, kind='scatter', color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here We'll use joinplot to compare the daily returns of Amazon and Microsoft\n",
    "sns.jointplot('AMZN', 'MSFT', returns_five, kind='scatter', color = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can see that if two stocks are perfectly (and positivley) correlated with each other a linear relationship bewteen its daily return values should occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are simply calling pairplot on our DataFrame for an automatic visual analysis \n",
    "# of all the comparisons\n",
    "sns.pairplot(returns_five, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using seabron for a quick correlation plot for the daily returns\n",
    "sns.heatmap(returns_five.corr(), annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Portfoilo optimization using Efficient Frontier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_close = data['Adj Close'][company_name]\n",
    "adjusted_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_return = adjusted_close.pct_change()\n",
    "daily_return.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing of Annual portfiolo return and risk assuming 20% weight on each stock\n",
    "#assuming trading days = 252 days in a year\n",
    "\n",
    "weights = np.array([0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "portfolio_return = np.sum(daily_return.mean()* weights)*252\n",
    "cov_matrix_annual = daily_return.cov()*252\n",
    "sns.heatmap(cov_matrix_annual, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_variance = np.dot(weights.T, np.dot(cov_matrix_annual,weights))\n",
    "portfolio_std = np.sqrt(portfolio_variance)\n",
    "risk_free_rate = 0.0178\n",
    "\n",
    "sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_std\n",
    "print(\"Expected annual return: \" + str(round((portfolio_return * 100),2))+'%')\n",
    "print(\"Expected Volatility: \" + str(round((portfolio_std * 100),2))+'%')\n",
    "print(\"Sharpe Ratio: \" + str(round((sharpe_ratio),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean daily return and covariance of daily returns\n",
    "mean_daily_returns = daily_return.mean()\n",
    "cov_matrix = daily_return.cov()\n",
    "\n",
    "#set number of runs of random portfolio weights\n",
    "num_portfolios = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_annualised_performance(weights, mean_returns, cov_matrix):\n",
    "    returns = np.sum(mean_returns*weights ) *252\n",
    "    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "    return std, returns\n",
    "  \n",
    "def random_portfolios(num_portfolios, mean_returns, cov_matrix, risk_free_rate):\n",
    "    results = np.zeros((3,num_portfolios))\n",
    "    weights_record = []\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(5)\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(weights, mean_returns, cov_matrix)\n",
    "        results[0,i] = portfolio_std_dev\n",
    "        results[1,i] = portfolio_return\n",
    "        results[2,i] = (portfolio_return - risk_free_rate) / portfolio_std_dev\n",
    "    return results, weights_record\n",
    "\n",
    "def neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate):\n",
    "    p_var, p_ret = portfolio_annualised_performance(weights, mean_returns, cov_matrix)\n",
    "    return -(p_ret - risk_free_rate) / p_var\n",
    "\n",
    "def max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix, risk_free_rate)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bound = (0.0,1.0)\n",
    "    bounds = tuple(bound for asset in range(num_assets))\n",
    "    result = sco.minimize(neg_sharpe_ratio, num_assets*[1./num_assets,], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result\n",
    "\n",
    "def portfolio_volatility(weights, mean_returns, cov_matrix):\n",
    "    return portfolio_annualised_performance(weights, mean_returns, cov_matrix)[0]\n",
    "\n",
    "def min_variance(mean_returns, cov_matrix):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bound = (0.0,1.0)\n",
    "    bounds = tuple(bound for asset in range(num_assets))\n",
    "\n",
    "    result = sco.minimize(portfolio_volatility, num_assets*[1./num_assets,], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "    return result\n",
    "\n",
    "def efficient_return(mean_returns, cov_matrix, target):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "\n",
    "    def portfolio_return(weights):\n",
    "        return portfolio_annualised_performance(weights, mean_returns, cov_matrix)[1]\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: portfolio_return(x) - target},\n",
    "                   {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0,1) for asset in range(num_assets))\n",
    "    result = sco.minimize(portfolio_volatility, num_assets*[1./num_assets,], args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result\n",
    "\n",
    "\n",
    "def efficient_frontier(mean_returns, cov_matrix, returns_range):\n",
    "    efficients = []\n",
    "    for ret in returns_range:\n",
    "        efficients.append(efficient_return(mean_returns, cov_matrix, ret))\n",
    "    return efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_calculated_ef_with_random(mean_returns, cov_matrix, num_portfolios, risk_free_rate):\n",
    "    results, _ = random_portfolios(num_portfolios,mean_returns, cov_matrix, risk_free_rate)\n",
    "    \n",
    "    max_sharpe = max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate)\n",
    "    sdp, rp = portfolio_annualised_performance(max_sharpe['x'], mean_returns, cov_matrix)\n",
    "    max_sharpe_allocation = pd.DataFrame(max_sharpe.x,index=adjusted_close.columns,columns=['allocation'])\n",
    "    max_sharpe_allocation.allocation = [round(i*100,2)for i in max_sharpe_allocation.allocation]\n",
    "    max_sharpe_allocation = max_sharpe_allocation.T\n",
    "\n",
    "    min_vol = min_variance(mean_returns, cov_matrix)\n",
    "    sdp_min, rp_min = portfolio_annualised_performance(min_vol['x'], mean_returns, cov_matrix)\n",
    "    min_vol_allocation = pd.DataFrame(min_vol.x,index=adjusted_close.columns,columns=['allocation'])\n",
    "    min_vol_allocation.allocation = [round(i*100,2)for i in min_vol_allocation.allocation]\n",
    "    min_vol_allocation = min_vol_allocation.T\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"Maximum Sharpe Ratio Portfolio Allocation\\n\")\n",
    "    print(\"Annualised Return:\" + str(round(rp*100,2)) + \"%\")\n",
    "    print(\"Annualised Volatility:\" + str(round(sdp*100,2)) + \"%\")\n",
    "    print(\"\\n\")\n",
    "    print(max_sharpe_allocation)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Minimum Volatility Portfolio Allocation\\n\")\n",
    "    print(\"Annualised Return:\" + str(round(rp_min * 100,2)) + \"%\")\n",
    "    print(\"Annualised Volatility:\" + str(round(sdp_min * 100,2)) + \"%\")\n",
    "    print(\"\\n\")\n",
    "    print(min_vol_allocation)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "   \n",
    "\n",
    "    plt.scatter(results[0,:],results[1,:],c=results[2,:],cmap='RdYlBu', marker='o', s=25, alpha=0.3)\n",
    "    plt.colorbar(label = 'Sharpe ratio')\n",
    "    plt.scatter(sdp,rp,marker=(5,1,0),color='r',s=100, label='Maximum Sharpe ratio')\n",
    "    plt.scatter(sdp_min,rp_min,marker=(5,1,0),color='g',s=100, label='Minimum volatility')\n",
    "\n",
    "    target = np.linspace(rp_min, 0.42, 50)\n",
    "    efficient_portfolios = efficient_frontier(mean_returns, cov_matrix, target)\n",
    "    plt.plot([p['fun'] for p in efficient_portfolios], target, linestyle='--', color='black', label='efficient frontier')\n",
    "    plt.title('Calculated Portfolio Optimization based on Efficient Frontier')\n",
    "    plt.xlabel('annualised volatility')\n",
    "    plt.ylabel('annualised returns')\n",
    "    plt.legend(labelspacing=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_calculated_ef_with_random(mean_daily_returns, cov_matrix, num_portfolios, risk_free_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the expected returns and the annualised sample covariance matrix of asset returns\n",
    "mu=expected_returns.mean_historical_return(adjusted_close)\n",
    "S= risk_models.sample_cov(adjusted_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For minimum volatility\n",
    "ef=EfficientFrontier(mu,S)\n",
    "weights=ef.min_volatility()\n",
    "cleaned_weights=ef.clean_weights()\n",
    "min_vol_port = ef.portfolio_performance(verbose=True)\n",
    "print(\"Allocation of weight\")\n",
    "for x, y in cleaned_weights.items():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n",
    "\n",
    "latest_prices = get_latest_prices(adjusted_close)\n",
    "weights = cleaned_weights\n",
    "\n",
    "da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=10000)\n",
    "allocation, leftover = da.lp_portfolio()\n",
    "print(\"Discrete allocation:\", allocation)\n",
    "print(\"Funds remaining: ${:.2f}\".format(leftover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How much value do we put at risk by investing in a particular stock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "(returns_five + 1).cumprod().plot()\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Growth of $1 investment\")\n",
    "plt.title(\"Monthly cumulative returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here e are defining a new DataFrame as a cleaned version of the original DataFrame\n",
    "rets = returns_five.dropna()\n",
    "\n",
    "area = np.pi*20\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(rets.mean(), rets.std(), s=area)\n",
    "plt.xlabel('Expected return')\n",
    "plt.ylabel('Risk')\n",
    "\n",
    "for label, x, y in zip(rets.columns, rets.mean(), rets.std()):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(50, 50), textcoords='offset points', ha='right', va='bottom', \n",
    "                 arrowprops=dict(arrowstyle='-', color='blue', connectionstyle='arc3,rad=-0.3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. How can we attempt to predict future stock behavior using LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the columns\n",
    "df = AAPL.iloc[:,0:6]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are Visualising the closing price history\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Close Price History')\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new data frame with only the closing price and convert it to an array. Then create a variable to store the length of the training data set. I want the training data set to contain about 70% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe with only the 'Close' column\n",
    "data = df.filter(['Close'])\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "dataset = data.values\n",
    "\n",
    "#Get /Compute the number of rows to train the model on\n",
    "training_data_len = math.ceil( len(dataset) *.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scale the data set to be values between 0 and 1 inclusive, I do this because it is generally good practice to scale your data before giving it to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# here we are Scaling the all of the data to be values between 0 and 1 \n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "scaled_data = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timestep and 1 output\n",
    "#Creating the scaled training data set\n",
    "# Note : Basically what we are trying to do here is take data from day 1 to 60 and make prediction of 61th days\n",
    "train_data = scaled_data[0:training_data_len  , : ]\n",
    "\n",
    "#Spliting the data into x_train and y_train data sets\n",
    "x_train=[]\n",
    "y_train = []\n",
    "for i in range(60,len(train_data)):\n",
    "    x_train.append(train_data[i-60:i,0])\n",
    "    y_train.append(train_data[i,0])\n",
    "    \n",
    "#Here we are Converting x_train and y_train to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Here we are reshaping the data into the shape accepted by the LSTM\n",
    "x_train_reshape = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "x_train_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are Building the LSTM network model\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences=True,input_shape=(x_train.shape[1],1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Addding the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='mean_squared_error')\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here we are training the model\n",
    "# model.fit(x_train, y_train, batch_size = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are testing data set\n",
    "test_data = scaled_data[training_data_len - 60: , : ]\n",
    "#Creating the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1233 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\n",
    "for i in range(60,len(test_data)):\n",
    "    x_test.append(test_data[i-60:i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are converting x_test to a numpy array  \n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# here we are reshaping the data into the shape accepted by the LSTM  \n",
    "x_test_reshape = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test_reshape, y_test)).batch(batch_size=64)\n",
    "history = model.fit(\n",
    "    x_train_reshape,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=val_dataset,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are getting the models predicted price values\n",
    "predictions = model.predict(x_test_reshape)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)#Undo scaling\n",
    "\n",
    "# here we are calculaing the value of Root Mean Square Error\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot/Create the data for the graph\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "#Visualize the data\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,predictions))\n",
    "print('Mean Square Error: ',metrics.mean_squared_error(y_test,predictions))\n",
    "print('Root Mean Square Error: ',math.sqrt(metrics.mean_squared_error(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(x_train, y_train)\n",
    "reg_pred = reg.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pred = reg_pred.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = scaler.inverse_transform(reg_pred)#Undo scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "#Visualize the data\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,predictions))\n",
    "print('Mean Square Error: ',metrics.mean_squared_error(y_test,predictions))\n",
    "print('Root Mean Square Error: ',math.sqrt(metrics.mean_squared_error(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
