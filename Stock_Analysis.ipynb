{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                            Stock Analysis and Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "    1. Introduction\n",
    "    2. Importing libraries\n",
    "    3. Reading datasets\n",
    "    4. Building Models\n",
    "    5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In general, Stock analysis is an assessment of a specific trading instrument, an investment segment, or the entire economy. It is a way of making purchasing and selling decisions for investors and traders. Investors and traders seek to gain an advantage in the markets by making educated choices by analyzing and reviewing past and present results.\n",
    "\n",
    "This project demonstrates the technique of machine learning called Long Short-Term Memory (LSTM) to forecast market prices and create portfoil optimization using Efficient Frontier. \n",
    "\n",
    "For this we will first evaluate the 20 different stocks and pick the top five higher return stocks that will be used to create a optimize portfoilo. Among them, one stock with a higher return and lower risk will be chosen to predict the future stock price.\n",
    "\n",
    "We will be answering the following question:\n",
    "\n",
    "1. What was the change in price of the stock over time?\n",
    "2. What was the monthly return of the stock on average?\n",
    "3. What was the moving average of the various stocks?\n",
    "4. What was the correlation between different stocks'?\n",
    "5. How much value do we put at risk by investing in a particular stock?\n",
    "6. Portfoilo optimization using Efficient Frontier?\n",
    "7. How can we attempt to predict future stock behavior using LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are list of all the libraries that are need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are importing important libraries\n",
    "import math\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as sco\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from termcolor import colored\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from pandas_datareader import data as web\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use(\"Solarize_Light2\")\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to carry out inventory analysis, we need historical asset data. There are several information providers, some of which are free, most of which are charged. For this project, we will be  using data from Yahoo's finance website since 2013. We can do this in Python with the pandas-datareader module.\n",
    "\n",
    "Companies names and their aliases (Ticker) that are used through out our Analysis\n",
    "    \n",
    "    1. Apple Inc. = AAPL\n",
    "    2. Alphabet Inc. = GOOG\n",
    "    3. Microsoft Corporation = MSFT\n",
    "    4. Amazon = AMZN\n",
    "    5. Facebook Inc. = FB\n",
    "    6. Alibaba Group = BABA\n",
    "    7. Johnson & Johnson = JNJ\n",
    "    8. JPMorgon Chase & Co. = JPM\n",
    "    9. ExxonMobil = XOM\n",
    "    10.Bank of America = BAC\n",
    "    11.WalMart Store Inc. = WMT\n",
    "    12.Wells Fargo & Co. = WFC\n",
    "    13.Visa Inc. = V\n",
    "    14.Procter & Gamble Co. = PG\n",
    "    15.Verizon Communication = VZ\n",
    "    16.AT&T Inc. = T\n",
    "    17.UnitedHealth Group Inc. = UNH\n",
    "    18.Home Depot = HD\n",
    "    19.Intel = INTC\n",
    "    20.Oracle = ORCL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of ticker of companies\n",
    "ticker_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'FB', 'BABA','JNJ', 'JPM', 'XOM', 'BAC', 'WMT', 'WFC', 'V', 'PG', 'VZ', 'T', 'UNH', 'HD', 'INTC', 'ORCL']\n",
    "\n",
    "# Mapping of Ticker with companies names\n",
    "CompanyNameMap={\n",
    "    \"AAPL\":\"Apple Inc\",\n",
    "    \"GOOG\":\"Alphabet Inc\",\n",
    "    \"MSFT\":\"Microsoft Corporation\",\n",
    "    \"AMZN\":\"Amazon\",\n",
    "    \"FB\":\"Facebook Inc\",\n",
    "    \"BABA\": \"Alibaba Group\",\n",
    "    \"JNJ\":\"Johnson & Johnson\",\n",
    "    \"JPM\":\"JPMorgon Chase & Co\",\n",
    "    \"XOM\": \"ExxonMobil\",\n",
    "    \"BAC\": \"Bank of America\",\n",
    "    \"WMT\": \"WalMart Store Inc\",\n",
    "    \"WFC\":\"Wells Fargo & Co\",\n",
    "    \"V\": \"Visa Inc\",\n",
    "    \"PG\": \"Procter & Gamble Co\",\n",
    "    \"VZ\": \"Verizon Communication\",\n",
    "    \"T\":\"AT&T Inc\",\n",
    "    \"UNH\": \"UnitedHealth Group Inc\",\n",
    "    \"HD\": \"Home Depot\",\n",
    "    \"INTC\": \"Intel\",\n",
    "    \"ORCL\":\"Oracle\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stocks data since 2013\n",
    "data = pdr.get_data_yahoo(ticker_list, start = '2015-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What was the change in price of the stock over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at historical data and then conducting the return calculations to see how well each stock has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the portion of historial data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the head of Monthly Adjused Close of companies\n",
    "monthly_adjusted_close = data['Adj Close'].resample('M').ffill()\n",
    "monthly_adjusted_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is quick summary of each company Monthly Adjusted closing\n",
    "monthly_adjusted_close.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the head of Monthly volume of companies\n",
    "monthly_volume = data['Volume'].resample('M').ffill()\n",
    "monthly_volume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What was the monthly return of the stock on average?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already gone through each company's historical data, the next thing to do is estimate the returns. We will calculate the returns on monthly rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating monthly return\n",
    "monthly_returns = data['Adj Close'].resample('M').ffill().pct_change()\n",
    "monthly_returns.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here we are visualising of Monthly Adjusted Price, Monthly Volume and Monthly Return\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 20, ncols = 3)\n",
    "#fig.suptitle('Monthly Adjusted Closing VS Monthly Volume VS Monthly Return')\n",
    "fig.set_figheight(80)\n",
    "fig.set_figwidth(15)\n",
    "plt.subplots_adjust(bottom=0.1, right=1, top=0.9)\n",
    "\n",
    "columns = list(monthly_adjusted_close) \n",
    "  \n",
    "for i, cols in enumerate(columns,0):\n",
    "    monthly_adjusted_close[cols].plot(ax = axes[i,0])\n",
    "    title = CompanyNameMap[ticker_list[i]]\n",
    "    axes[i,0].set(xlabel='Date', ylabel='Adj Close')\n",
    "    axes[i,0].set_title(f\"{title}\")\n",
    "    axes[i,0].set_ylim(0, 3500)\n",
    "    monthly_volume[cols].plot(ax = axes[i,1])\n",
    "    axes[i,1].set(xlabel='Date', ylabel='Volume')\n",
    "    axes[i,1].set_title(f\"{title}\")\n",
    "    axes[i,1].set_ylim(550000, 450000000)\n",
    "    monthly_returns[cols].plot(ax = axes[i,2])\n",
    "    axes[i,2].set(xlabel='Date', ylabel='Monthly Return')\n",
    "    axes[i,2].set_title(f\"{title}\")\n",
    "    axes[i,2].set_ylim(-0.25, 0.5)\n",
    "    \n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "print(colored('\\t\\t\\tMonthly Adjusted Closing VS Monthly Volume VS Monthly % Change', attrs=['bold']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above graph the first column figure shows monthly adjusted close price patterns, the second column graph shows the monthly volumes traded and the last column shows the monthly returns of each stock.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualization of Average Monthly Return\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(columns, monthly_returns.mean())\n",
    "\n",
    "# zip joins x and y coordinate in pair\n",
    "for x,y in zip(columns,monthly_returns.mean()):\n",
    "    label = \"{:.3f}\".format(y)\n",
    "    plt.annotate(label,(x,y),textcoords = 'offset points',xytext = (0,10),ha = 'center')\n",
    "\n",
    "plt.xlabel('Company Symbols')\n",
    "plt.ylabel('Average monthly return')\n",
    "plt.suptitle('Average Monthly Return', fontweight = 'bold', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we can infer after looking at the average monthly return chart is that Amazon has the highest monthly average return with 3.5 percent, while ExxonMobil has the lowest monthly average return with -0.4 percent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's filter the stocks down to top 5 with a higher monthly average return and look at the graph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of top five stock with highest monthly return\n",
    "top_five = monthly_returns.mean().nlargest(5)\n",
    "\n",
    "company_name = list(top_five.keys())\n",
    "names = []\n",
    "for key in company_name:\n",
    "    names.append(CompanyNameMap[key])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(names, top_five)\n",
    "\n",
    "# zip joins x and y coordinate in pair\n",
    "for x,y in zip(names,top_five):\n",
    "    label = \"{:.3f}\".format(y)\n",
    "    plt.annotate(label,\n",
    "                (x,y),\n",
    "                textcoords = 'offset points',\n",
    "                xytext = (0,10),\n",
    "                ha = 'center')\n",
    "plt.suptitle('Top 5 companies with highest average monthly return', fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel('Average Monthly returns')\n",
    "plt.xlabel('Companies name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we've filtered the companies to top 5 on the basis of average monthly return and have seen the visualizations for the Adjusted price and the volume traded each day with monthly percentage change, let's go ahead and calculate the moving average for the stock.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. What was the moving average of the various stocks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will be adding three major moving average intervals to our graphs in order to allow us to trade better. They are the moving averages of 10, 20 and 50 days.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using loop for grabing yahoo data and setting it in form of dataframe\n",
    "topFiveDataDict={}\n",
    "for stock in company_name:\n",
    "    topFiveDataDict[stock] = web.DataReader(stock,\"yahoo\",'2015-01-01',datetime.now())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are making pandas dataframe for top 5 company stocks\n",
    "company_list = topFiveDataDict.values()\n",
    "for company, comp_name in zip(company_list,company_name):\n",
    "    company[\"company_name\"] = comp_name\n",
    "    \n",
    "company_data = pd.concat(company_list,axis=0)\n",
    "company_data.tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the moving averages\n",
    "ma_day = [10, 20, 50]\n",
    "\n",
    "for ma in ma_day:\n",
    "    for company in company_list:\n",
    "        column_name = f\"MA for {ma} days\"\n",
    "        company[column_name] = company['Adj Close'].rolling(ma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are visualising three important moving averages of all the company\n",
    "    \n",
    "Total = 5\n",
    "Columns = 2\n",
    "\n",
    "# array of company datas\n",
    "datas = list(topFiveDataDict.values())\n",
    "\n",
    "# names of the company in array\n",
    "names = list(topFiveDataDict.keys())\n",
    "\n",
    "# Compute Rows required\n",
    "Rows = Total // Columns \n",
    "Rows += Total % Columns\n",
    "\n",
    "# Create a Position index\n",
    "Position = range(1,Total + 1)\n",
    "fig = plt.figure(1)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "for k in range(Total):\n",
    "  # add every single subplot to the figure with a for loop\n",
    "    ax = fig.add_subplot(Rows,Columns,Position[k])\n",
    "    ax.plot(datas[k]['Adj Close'])\n",
    "    ax.plot(datas[k]['MA for 10 days'], color = 'yellow')\n",
    "    ax.plot(datas[k]['MA for 20 days'], color = 'green')\n",
    "    ax.plot(datas[k]['MA for 50 days'], color = 'orange')\n",
    "    ax.set_title(CompanyNameMap[names[k]])   \n",
    "    ax.legend(['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days'])\n",
    "    ax.set(xlabel='Date',ylabel='Moving Average')\n",
    "\n",
    "fig.tight_layout(pad=1.5)\n",
    "fig.suptitle('Moving Average of top 5 companies', fontsize = 16, fontweight = 'bold', y = 1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can conclude that the average trend is identical to the patterns of its Adjusted Close Price regardless of any number of days. Microsoft seems to be the one that most of the time indicates the rising pattern, while Alibaba has the most fluctuating trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. What is the correlation between different stocks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Top five company Monthly Adjusted Close price\n",
    "adjusted_close_five= monthly_adjusted_close[company_name]\n",
    "adjusted_close_five.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are Making DataFrame which is Monthly % change\n",
    "returns_five= adjusted_close_five.pct_change()\n",
    "returns_five.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use joinplot to compare the monthly return of Amazon to itself and also between Amazon to Microsoft. Correlation between Amazon to itself should shows a perfectly linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Amazon to itself should show a perfectly linear relationship\n",
    "fig = sns.jointplot('AMZN', 'AMZN', returns_five, kind='scatter', color = \"blue\")\n",
    "fig.set_axis_labels('Amazon','Amazon')\n",
    ";\n",
    "print(colored('\\tCorrelation between Amazon with Amazon', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing monthly returns of Amazon and Microsoft\n",
    "fig = sns.jointplot('AMZN', 'MSFT', returns_five, kind='scatter', color = \"blue\")\n",
    "fig.set_axis_labels('Amazon','Microsoft')\n",
    ";\n",
    "print(colored('\\tCorrelation between Amazon with Microsoft', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if two stocks are perfectly (and positivley) correlated with each other a linear relationship between its monthly return values should occur. So now let see the relation between all the stock with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns \n",
    "fullname = []\n",
    "for name in company_name:\n",
    "    fullname.append(CompanyNameMap[name])\n",
    "\n",
    "returns_five.columns = fullname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are calling pairplot on our DataFrame for an automatic visual analysis of all the comparisons\n",
    "sns.pairplot(returns_five)\n",
    ";\n",
    "print(colored('\\t\\t\\t\\t\\tCorrelation between 5 companies', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that all stocks are positively correlated and that Microsoft and Amazon had the highest monthly stock return correlation, while Alibaba and Apple were the least correlated. Let's see its meaning in numerical terms to understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are using seaborn for a quick correlation plot for the monthly returns\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.heatmap(returns_five.corr(), annot=True, cmap=\"YlGnBu\")\n",
    "plt.title('Correlation between 5 companies', fontsize = 14, fontweight = 'bold', y = 1.05)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How much value do we put at risk by investing in a particular stock?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It cannot be fruitful to invest in a stock that focuses only on higher returns, ignoring its risk factor. The best investment decision is known to be inventory with high return with low risk. So we'll look at the volatility of the stock from now on as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualiation of Monthly Return percent\n",
    "\n",
    "#Getting columns heading\n",
    "columns = list(returns_five) \n",
    "\n",
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "for i, col in enumerate(columns, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.plot(returns_five[col])\n",
    "    plt.ylabel('Monthly Return')\n",
    "    plt.title(f'{CompanyNameMap[names[i - 1]]}')\n",
    "    plt.ylim(-0.25,0.45)\n",
    "    \n",
    "plt.suptitle('Monthly Return Data\\n\\n', fontweight = 'bold', fontsize = 14, y = 0.93)\n",
    "plt.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the monthly returns chart of each stock, we can conclude that the returns are quite volatile and the stock can move +0.5  to - 0.3% on any given month. To get a sense of how extreme the returns can be we can plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualiation of Monthly Return percent\n",
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "for i, col in enumerate(columns, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.distplot(returns_five[col].dropna(), bins=100, color='blue')\n",
    "    plt.ylabel('Monthly Return')\n",
    "    plt.xlabel('Monthly Return %')\n",
    "    plt.title(f'{CompanyNameMap[names[i - 1]]}')\n",
    "    plt.ylim(0,25)\n",
    "    plt.xlim(-0.4,0.65)\n",
    "    plt.text(-0.3, 15, 'Extreme Low\\n Return', fontsize = 12)\n",
    "    plt.text(0.3, 15, 'Extreme high\\n Return', fontsize = 12)\n",
    "    \n",
    "plt.suptitle('Monthly Return Data\\n\\n', fontweight = 'bold', fontsize = 14, y = 0.93)\n",
    "plt.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that Microsoft's monthly return is least volatile and while Alibaba's is highly volatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting monthly returns are useful for understanding the monthly volatility of the investment. To calculate the growth of our investment or in other word, calculating the total returns from our investment, we need to calculate the cumulative returns from that investment. To calculate the cumulative returns we will use the cumprod() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the cumulative return\n",
    "cum_returns = (returns_five + 1).cumprod()\n",
    "values = cum_returns.tail(1).values[0]\n",
    "values = map(lambda x : \"$ {:.2f}\".format(x), values)\n",
    "\n",
    "#Visualizing the cumulative return of stock\n",
    "ax = cum_returns.plot(figsize = (10,8))\n",
    "ax.set_title(\"Monthly cumulative returns\", fontsize = 14, fontweight = 'bold', y = 1.05)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Growth of $1 investment\")\n",
    "for line, name in zip(ax.lines, values):\n",
    "    y = line.get_ydata()[-1]\n",
    "    ax.annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \n",
    "                xycoords = ax.get_yaxis_transform(), textcoords=\"offset points\",\n",
    "                size=14, va=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart displays the cumulative returns of each stock since 2015. It is showing how much one could have made today with an investment of $1 in 2015. Not surprisingly, from 2015, Amazon has had the highest returns. In the distant second and third, Microsoft and Apple come in. While its lowest earner is Alibaba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are defining a new DataFrame as a cleaned version of the original DataFrame\n",
    "rets = returns_five.dropna()\n",
    "\n",
    "area = np.pi*20\n",
    "\n",
    "#Visualizing the cumulative return of stock\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(rets.mean(), rets.std(), s=area)\n",
    "plt.xlabel('Expected return')\n",
    "plt.ylabel('Risk')\n",
    "plt.title('Risk and Expected Return',fontsize = 14, fontweight = 'bold', y = 1.25)\n",
    "\n",
    "for label, x, y in zip(rets.columns, rets.mean(), rets.std()):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(50, 50), textcoords='offset points', ha='right', va='bottom', \n",
    "                 arrowprops=dict(arrowstyle='-', color='blue', connectionstyle='arc3,rad=-0.3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above diagram shows the risk-return relationship of the best five high monthly return stock. It shows that Alibaba, Facebook and Apple Inc has high risk and low return compared to Microsoft and Amazon,  which clearly eliminate this stock on the proposed  investment decision. Amazon showing the highest return stock among them with 3.4% monthly return but considering the risk of 8.5% shows that the Amazon is good stock for investment for aggressive risk taker. As the conservative risk taker, Microsoft is the best stock with low level of risk of 6% to obtain the return of 2.8% is best stock to invest among these stock."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Portfoilo optimization using Efficient Frontier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the same top five stock with a higher return. We allocated equal weights of 20 percent to each of for the formation of the portfolio and measure the return and risk of investing in that portfolio. After that, using the minimum Sharpe ratio and minimum variance (risk or standard deviation), we determined the effective frontier portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Adjuested Close Price of five selected sock\n",
    "adjusted_close = data['Adj Close'][company_name]\n",
    "\n",
    "# Changing the columns name\n",
    "adjusted_close.columns = fullname\n",
    "adjusted_close.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the daily return\n",
    "daily_return = adjusted_close.pct_change()\n",
    "daily_return.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing of Annual portfiolo return and risk assuming 20% weight on each stock\n",
    "#assuming trading days = 252 days in a year\n",
    "\n",
    "weights = np.array([0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "# Calculating annual return of our portfolio\n",
    "portfolio_return = np.sum(daily_return.mean()* weights)*252\n",
    "\n",
    "# Calculation of covariance matrix\n",
    "cov_matrix_annual = daily_return.cov()*252\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of covariance matrix\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.heatmap(cov_matrix_annual, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title('Covariance of 5 companies', fontsize = 14, fontweight = 'bold', y = 1.05)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above covariance matrix we can conclude that among the top five stocks, Alibabas companys stock has the highest volatility and combination of Alibaba and Appple has least volatility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the volatility of our portfolio\n",
    "portfolio_variance = np.dot(weights.T, np.dot(cov_matrix_annual,weights))\n",
    "portfolio_std = np.sqrt(portfolio_variance)\n",
    "\n",
    "# Assuming the value of risk free rate that already exist in market\n",
    "risk_free_rate = 0.0178\n",
    "\n",
    "# Calculation of sharpe ratio\n",
    "# Sharpe ratio shows how much additional return an investor earns by taking additional risk\n",
    "sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we invest 20% of our total on each asset, then our portfolio will give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Expected annual return: \" + str(round((portfolio_return * 100),2))+'%')\n",
    "print(\"Expected Volatility: \" + str(round((portfolio_std * 100),2))+'%')\n",
    "print(\"Sharpe Ratio: \" + str(round((sharpe_ratio),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know the projected return and volatility of our current portfolio. But what if we are not satisfied with our current portfolio's level of volatility and would like to reduce it? In pursuit of a higher expected return, what if we are willing to take on more risk? To achieve these objectives, how do we rearrange the weight of each stock in our portfolio? In this project, our main objective is to obtain an optimal portfolio. Two portfolios that we may like to highlight are\n",
    "    \n",
    "    1. The portfolio with the highest Sharpe Ratio (i.e. the highest risk adjusted returns) and \n",
    "    2. The \"minimum variance portfolio,\" which is the lowest volatility portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean daily return and covariance of daily returns\n",
    "mean_daily_returns = daily_return.mean()\n",
    "cov_matrix = daily_return.cov()\n",
    "\n",
    "#set number of runs of random portfolio weights\n",
    "num_portfolios = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will calculate the annual return and volatility\n",
    "def portfolio_annualised_performance(weights, mean_returns, cov_matrix):\n",
    "    returns = np.sum(mean_returns*weights ) *252\n",
    "    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "    return std, returns\n",
    "  \n",
    "# function that will generate portfolios with random weight\n",
    "def random_portfolios(num_portfolios, mean_returns, cov_matrix, risk_free_rate):\n",
    "    results = np.zeros((3,num_portfolios))\n",
    "    weights_record = []\n",
    "    for i in range(num_portfolios):\n",
    "        weights = np.random.random(5)\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(weights, mean_returns, cov_matrix)\n",
    "        results[0,i] = portfolio_std_dev\n",
    "        results[1,i] = portfolio_return\n",
    "        results[2,i] = (portfolio_return - risk_free_rate) / portfolio_std_dev\n",
    "    return results, weights_record\n",
    "\n",
    "#Sharpe ratio describes how much excess return you are receiving for the extra volatility that you endure for holding a riskier asset. \n",
    "# function to calculate minimum sharpe ratio\n",
    "def neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate):\n",
    "    p_var, p_ret = portfolio_annualised_performance(weights, mean_returns, cov_matrix)\n",
    "    return -(p_ret - risk_free_rate) / p_var\n",
    "\n",
    "#function to calculate maximum sharpe ratio\n",
    "def max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix, risk_free_rate)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bound = (0.0,1.0)\n",
    "    bounds = tuple(bound for asset in range(num_assets))\n",
    "    result = sco.minimize(neg_sharpe_ratio, num_assets*[1./num_assets,], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result\n",
    "\n",
    "# function to calculate annual volatility\n",
    "def portfolio_volatility(weights, mean_returns, cov_matrix):\n",
    "    return portfolio_annualised_performance(weights, mean_returns, cov_matrix)[0]\n",
    "\n",
    "# function to calculate minimum variance\n",
    "def min_variance(mean_returns, cov_matrix):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bound = (0.0,1.0)\n",
    "    bounds = tuple(bound for asset in range(num_assets))\n",
    "\n",
    "    result = sco.minimize(portfolio_volatility, num_assets*[1./num_assets,], args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "    return result\n",
    "\n",
    "# function that calculate the most efficient portfolio for a gieb target return\n",
    "def efficient_return(mean_returns, cov_matrix, target):\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "\n",
    "    def portfolio_return(weights):\n",
    "        return portfolio_annualised_performance(weights, mean_returns, cov_matrix)[1]\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: portfolio_return(x) - target},\n",
    "                   {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0,1) for asset in range(num_assets))\n",
    "    result = sco.minimize(portfolio_volatility, num_assets*[1./num_assets,], args=args, method='SLSQP', bounds=bounds, constraints=constraints)# Sequential Least Squares Programming (SLSQP).\n",
    "    return result\n",
    "\n",
    "# function that will take a range of target return ana compute efficient portfolio of each return level\n",
    "def efficient_frontier(mean_returns, cov_matrix, returns_range):\n",
    "    efficients = []\n",
    "    for ret in returns_range:\n",
    "        efficients.append(efficient_return(mean_returns, cov_matrix, ret))\n",
    "    return efficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try to plot the portfolio choices with maximum Sharpe ratio and minimum volatility also with all the randomly generated portfolios. But this time, we are not picking the optimal ones from the randomly generated portfolios, but we are actually calculating by using Scipy’s ‘minimize’ function. And the below function will also plot the efficient frontier line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_calculated_ef_with_random(mean_returns, cov_matrix, num_portfolios, risk_free_rate):\n",
    "    results, _ = random_portfolios(num_portfolios,mean_returns, cov_matrix, risk_free_rate)\n",
    "    \n",
    "    max_sharpe = max_sharpe_ratio(mean_returns, cov_matrix, risk_free_rate)\n",
    "    sdp, rp = portfolio_annualised_performance(max_sharpe['x'], mean_returns, cov_matrix)\n",
    "    max_sharpe_allocation = pd.DataFrame(max_sharpe.x,index=adjusted_close.columns,columns=['allocation'])\n",
    "    max_sharpe_allocation.allocation = [round(i*100,2)for i in max_sharpe_allocation.allocation]\n",
    "    max_sharpe_allocation = max_sharpe_allocation.T\n",
    "\n",
    "    min_vol = min_variance(mean_returns, cov_matrix)\n",
    "    sdp_min, rp_min = portfolio_annualised_performance(min_vol['x'], mean_returns, cov_matrix)\n",
    "    min_vol_allocation = pd.DataFrame(min_vol.x,index=adjusted_close.columns,columns=['allocation'])\n",
    "    min_vol_allocation.allocation = [round(i*100,2)for i in min_vol_allocation.allocation]\n",
    "    min_vol_allocation = min_vol_allocation.T\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(colored('Maximum Sharpe Ratio Portfolio Allocation\\n', attrs=['bold']))\n",
    "    print(\"Annualised Return:\" + str(round(rp*100,2)) + \"%\")\n",
    "    print(\"Annualised Volatility:\" + str(round(sdp*100,2)) + \"%\")\n",
    "    print(\"\\n\")\n",
    "    print(max_sharpe_allocation)\n",
    "    print(\"-\"*80)\n",
    "    print(colored('Minimum Volatility Portfolio Allocation\\n', attrs=['bold']))\n",
    "    print(\"Annualised Return:\" + str(round(rp_min * 100,2)) + \"%\")\n",
    "    print(\"Annualised Volatility:\" + str(round(sdp_min * 100,2)) + \"%\")\n",
    "    print(\"\\n\")\n",
    "    print(min_vol_allocation)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "   \n",
    "\n",
    "    plt.scatter(results[0,:],results[1,:],c=results[2,:],cmap='RdYlBu', marker='o', s=25, alpha=0.3)\n",
    "    plt.colorbar(label = 'Sharpe ratio')\n",
    "    plt.scatter(sdp,rp,marker=(5,1,0),color='r',s=100, label='Maximum Sharpe ratio')\n",
    "    plt.scatter(sdp_min,rp_min,marker=(5,1,0),color='g',s=100, label='Minimum volatility')\n",
    "\n",
    "    target = np.linspace(rp_min, 0.42, 30)\n",
    "    efficient_portfolios = efficient_frontier(mean_returns, cov_matrix, target)\n",
    "    plt.plot([p['fun'] for p in efficient_portfolios], target, linestyle='--', color='black', label='efficient frontier')\n",
    "    plt.title('Calculated Portfolio Optimization based on Efficient Frontier', fontsize = 14, fontweight = 'bold', y = 1.05)\n",
    "    plt.xlabel('annualised volatility')\n",
    "    plt.ylabel('annualised returns')\n",
    "    plt.legend(labelspacing=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the information and visualaizating of portfolios\n",
    "display_calculated_ef_with_random(mean_daily_returns, cov_matrix, num_portfolios, risk_free_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the low risk portfolio, we can see that Microsoft and Apple are given a large budget. If you take another look at the monthly return plot from earlier, you can see that the least volatile stock of five is Microsoft and Apple, so it makes intuitive sense to assign a large percentage to Microsoft and then Apple for the minimum risk portfolio.\n",
    "\n",
    "If we are able to take a higher risk for a higher return, the maximum Sharpe ratio is one that gives us the best risk-adjusted return. We allocate a large portion to Amazon in this situation, which are fairly volatile stocks from the previous monthly returns map. Microsoft and Apple, which had approximately 50 percent in the case of the low risk portfolio, have allocated the remaining budget to it. Leaving Facebook and Alibaba with 0% alloaction.\n",
    "\n",
    "We can see that it forms the shape of an arch line on the top of clustered blue dots from the plot of the randomly simulated portfolios. This line is called the efficient frontier. Why is that efficient? Since points along the line will give you the lowest risk of return for a given target return. All the other dots on the line would give you higher risk with the same returns.\n",
    "\n",
    "The way we found the two kinds of optimal portfolio above was by simulating many possible random choices and pick the best ones (either minimum risk or maximum risk-adjusted return)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the minimun volatility portfolio, if we have $10000, lets see much stock we can buy in todays price for the top 5 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the expected returns and the annualised sample covariance matrix of asset returns\n",
    "mu=expected_returns.mean_historical_return(adjusted_close)\n",
    "S= risk_models.sample_cov(adjusted_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For minimum volatility\n",
    "ef=EfficientFrontier(mu,S)\n",
    "weights=ef.min_volatility()\n",
    "cleaned_weights=ef.clean_weights()\n",
    "min_vol_port = ef.portfolio_performance(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n",
    "\n",
    "latest_prices = get_latest_prices(adjusted_close)\n",
    "weights = cleaned_weights\n",
    "\n",
    "da = DiscreteAllocation(weights, latest_prices, total_portfolio_value=10000)\n",
    "allocation, leftover = da.lp_portfolio()\n",
    "print(\"Discrete allocation:\", allocation)\n",
    "print(\"Funds remaining: ${:.2f}\".format(leftover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. How can we attempt to predict future stock behavior using LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data of only Microsoft\n",
    "df = topFiveDataDict['MSFT']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "df.isna().any()\n",
    "df = df.iloc[:,0:6]\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are Visualising the closing price history\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Close Price History',fontsize = 14, fontweight = 'bold', y = 1.02)\n",
    "plt.plot(df['Close'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From above graph we can see that microsofts closing price is in increasing trend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get close stock data\n",
    "df1 = df[['Close']]\n",
    "#reset index of data frame\n",
    "df1.reset_index(inplace=True)\n",
    "#converting date to pandas datetime format\n",
    "df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "\n",
    "# Creating pivot table for monts\n",
    "month_table = pd.pivot_table(df1,index=df1['Date'].dt.strftime('%b'),\n",
    "               values=[\"Close\"],\n",
    "               aggfunc='mean',fill_value=0)\n",
    "\n",
    "# Sorting by month\n",
    "month = ['Jan', 'Feb', 'Mar', 'Apr','May','Jun', 'Jul', 'Aug','Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_table.index = pd.CategoricalIndex(month_table.index, categories=month, ordered=True)\n",
    "month_table = month_table.sort_index()\n",
    "\n",
    "\n",
    "# Creating pivot table for weeks\n",
    "weekdays_table = pd.pivot_table(df1,index=df1['Date'].dt.strftime('%a'),\n",
    "               values=[\"Close\"],\n",
    "               aggfunc='mean',fill_value=0)\n",
    "\n",
    "weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "weekdays_table.index = pd.CategoricalIndex(weekdays_table.index, categories=weekdays, ordered=True)\n",
    "weekdays_table = weekdays_table.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(month_table['Close'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2,figsize=(10,5))\n",
    "\n",
    "month_table.plot(kind = 'bar', ax = ax[0], width=0.75)\n",
    "ax[0].set_title('Average Monthly Price', fontweight = 'bold', fontsize = 15, y = 1.02)\n",
    "ax[0].set_xlabel('Months')\n",
    "ax[0].set_ylabel('Price')\n",
    "ax[0].get_legend().remove()\n",
    "\n",
    "for rect in ax[0].patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.1f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax[0].annotate(label,                    \n",
    "            (x_value, y_value),         \n",
    "            xytext=(0, 3),         \n",
    "            textcoords=\"offset points\", \n",
    "            ha='center') \n",
    "\n",
    "\n",
    "\n",
    "weekdays_table.plot(kind = 'bar', ax = ax[1])\n",
    "ax[1].set_title('Average Weekdays Price', fontweight = 'bold', fontsize = 15, y = 1.02)\n",
    "ax[1].set_xlabel('Weekdays')\n",
    "ax[1].set_ylabel('Price')\n",
    "ax[1].get_legend().remove()\n",
    "\n",
    "# Code for displaying values above bar\n",
    "for rect in ax[1].patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.1f}\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        ax[1].annotate(label,                    \n",
    "            (x_value, y_value),         \n",
    "            xytext=(0, 3),         \n",
    "            textcoords=\"offset points\", \n",
    "            ha='center')              \n",
    "\n",
    "plt.subplots_adjust(bottom=0.1, right=1.8, top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above weekly graph we can see that stock price is are tentatively similar through out the week days but is slighly higher in Monday.\n",
    "\n",
    "If we look at monthly graph we can see that november is the month with the highest stock price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Create a new data frame with only the closing price and convert it to an array. Then create a variable to store the length of the training data set. I want the training data set to contain about 80% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe with only the 'Close' column\n",
    "data = df.filter(['Close'])\n",
    "\n",
    "#Converting the dataframe to a numpy array\n",
    "dataset = data.values\n",
    "\n",
    "#Get /Compute the number of rows to train the model on\n",
    "training_data_len = math.ceil( len(dataset) *.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scale the data set to be values between 0 and 1 inclusive, I do this because it is generally good practice to scale your data before giving it to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling, Here we are Scaling the all of the data to be values between 0 and 1 using MinMax Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "scaled_data = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a data structure with 60 timestep and 1 output and then split the data into x_train and y_train data sets. Basically what we are trying to do here is take data from day 1 to 60 and make prediction of 61th days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the scaled training data set\n",
    "train_data = scaled_data[0:training_data_len  , : ]\n",
    "\n",
    "timestep = 60\n",
    "x_train=[]\n",
    "y_train = []\n",
    "\n",
    "#Splitting data into x_train and y_train\n",
    "for i in range(timestep,len(train_data)):\n",
    "    x_train.append(train_data[i-timestep:i,0])\n",
    "    y_train.append(train_data[i,0])\n",
    "    \n",
    "#Here we are Converting x_train and y_train to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Here we are reshaping the data into the shape accepted by the LSTM\n",
    "x_train_reshape = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are testing data set and again repeat the same process that we did for x_train\n",
    "test_data = scaled_data[training_data_len - timestep: , : ]\n",
    "\n",
    "#Creating the x_test and y_test data sets\n",
    "x_test = []\n",
    "y_test =  dataset[training_data_len : , : ] # Here we fit the actual data that is lastest 30% data \n",
    "\n",
    "for i in range(timestep,len(test_data)):\n",
    "    x_test.append(test_data[i-timestep:i,0])\n",
    "    \n",
    "# here we are converting x_test to a numpy array  \n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# here we are reshaping the data into the shape accepted by the LSTM  \n",
    "x_test_reshape = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are Building the LSTM network model\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 128, return_sequences=True, activation='tanh', input_shape=(x_train.shape[1],1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the second LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 64, return_sequences=True, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the third LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units = 32, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Addding the output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Here we are training the model\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test_reshape, y_test)).batch(batch_size=64)\n",
    "history = model.fit(\n",
    "    x_train_reshape,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_data=val_dataset,\n",
    "    use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are getting the models predicted price values\n",
    "predictions = model.predict(x_test_reshape)\n",
    "\n",
    "predictions = scaler.inverse_transform(predictions)#Undo scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the data for the graph\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "\n",
    "# Table of predicted and actual price\n",
    "valid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the data\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('Model generated price prediction',fontsize = 14, fontweight = 'bold', y = 1.02)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid['Close'], color = 'green')\n",
    "plt.plot(valid['Predictions'], color = 'orange')\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title('Predicted Price vs Actual Price',fontsize = 14, fontweight = 'bold', y = 1.02)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(valid['Close'], color = 'green')\n",
    "plt.plot(valid['Predictions'], color = 'orange')\n",
    "plt.legend(['Actual Price', 'Predictions Price'], loc='lower right')\n",
    "plt.show()\n",
    ";\n",
    "\n",
    "print('Mean Absolute Error: {:.3f}'.format(metrics.mean_absolute_error(y_test,predictions)))\n",
    "print('Mean Square Error: {:.3f}'.format(metrics.mean_squared_error(y_test,predictions)))\n",
    "print('Root Mean Square Error: {:.3f}'.format(math.sqrt(metrics.mean_squared_error(y_test,predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can infer from the above graph is that our model does the fine job of predicting the stock price as the real price and the expected price are very similar with just 6.46 percent of the mean absolute error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already developed our model using LSTM for stock price prediction. Now, what we are planning to try is to forecast stock prices for the next 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking previous 60 data in order to predict future stock price\n",
    "x_input = test_data[len(test_data)-timestep:].reshape(1,-1)\n",
    "\n",
    "temp_input = list(x_input)\n",
    "temp_input = temp_input[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction for next 30 days\n",
    "\n",
    "lst_output=[] # list to store the 30 days predicted price\n",
    "n_steps = 60\n",
    "i = 0\n",
    "while(i < 30):\n",
    "    \n",
    "    if(len(temp_input) > 60):\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        x_input = x_input.reshape(1,-1)\n",
    "        x_input = x_input.reshape((1, n_steps, 1))\n",
    "        yhat = model.predict(x_input) # predicting the price\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        temp_input=temp_input[1:]\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i = i + 1\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps,1))\n",
    "        yhat = model.predict(x_input) # predicting the price\n",
    "        temp_input.extend(yhat[0].tolist())\n",
    "        lst_output.extend(yhat.tolist())\n",
    "        i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions = scaler.inverse_transform(lst_output)#Undo scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "lst_date = [] # list to that store next 30 days\n",
    "day = datetime.datetime.today() # today date\n",
    "\n",
    "# Getting next 30 days\n",
    "for i in range(30):\n",
    "    day = day + datetime.timedelta(days=1)\n",
    "    nextDay = day.strftime ('%Y-%m-%d') # format the date to ddmmyyyy\n",
    "    lst_date.append(nextDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting todays date\n",
    "today = datetime.datetime.today().strftime ('%Y-%m-%d')\n",
    "\n",
    "# Getting today price of stock\n",
    "today_price = df['Close'].tail(1).values\n",
    "\n",
    "# Creating dataframe today stock price\n",
    "today_df = pd.DataFrame({'Date': today, 'Price':today_price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the predicted stock price to dataframe\n",
    "pred_df = pd.DataFrame({'Date':lst_date})\n",
    "\n",
    "pred_df['Price'] = future_predictions\n",
    "\n",
    "pred_df = pd.concat([today_df,pred_df])\n",
    "\n",
    "pred_df['Date'] = pd.to_datetime(pred_df['Date'])\n",
    "\n",
    "pred_df = pred_df.set_index('Date')\n",
    "\n",
    "# Looking at the head of predicted price\n",
    "pred_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the price data\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(df['Close'].tail(150))\n",
    "plt.plot(pred_df)\n",
    "plt.legend(['Past Price', 'Future Price'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our model prediction shows that the stock price will be in increasing trend for the next 30 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of what we did in this project\n",
    "- Took 20 companies stock data since 2013 from yahoo finance\n",
    "- Filtered 20 companies to top 5 on the basis of monthly return on investment\n",
    "- Made optional portfolio of top 5 companies using [efficient frontier](https://www.investopedia.com/terms/e/efficientfrontier.asp#:~:text=The%20efficient%20frontier%20is%20the,for%20the%20level%20of%20risk.)\n",
    "- Selected one company among the top 5 on the basis of minimum risk and maximun return\n",
    "- Used LSTM Neural Network model to predict the price of stock for next 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
